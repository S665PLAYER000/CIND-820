{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credit Risk Classification Using Machine Learning  \n",
        "**CIND 820 â€“ Capstone Project**  \n",
        "Strahinja Nakic | Toronto Metropolitan University | 500809487"
      ],
      "metadata": {
        "id": "AJmGZzUIfFpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "\n",
        "This project uses the German Credit dataset to classify loan applicants as good or bad credit risks.\n",
        "The analysis applies three supervised machine learning models: Logistic Regression, Decision Tree, and Naive Bayes.\n",
        "These models are trained and evaluated using performance metrics such as accuracy, recall, confusion matrix, and cross-validation"
      ],
      "metadata": {
        "id": "0sKj-j8jcpJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing the Libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report\n",
        "import pprint\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "lc-nXspYcuo4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive and Loading Dataset\n",
        "\n",
        "The dataset is stored in Google Drive and loaded using pandas.\n",
        "Column names are manually defined since the dataset has no header row\n"
      ],
      "metadata": {
        "id": "jH8FnQaRc8kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Mounting my Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "## Loading the dataset from Google Drive\n",
        "file_path = '/content/drive/My Drive/CIND 820/german.data' ## Replace with your dataset location\n",
        "\n",
        "column_names = [\n",
        "    \"checking_account\", \"duration\", \"credit_history\", \"purpose\", \"credit_amount\",\n",
        "    \"savings_account\", \"employment\", \"installment_rate\", \"personal_status\",\n",
        "    \"other_debtors\", \"residence_since\", \"property\", \"age\", \"other_installment_plans\",\n",
        "    \"housing\", \"existing_credits\", \"job\", \"people_liable\", \"telephone\",\n",
        "    \"foreign_worker\", \"credit_risk\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(file_path, sep=' ', header=None, names=column_names)\n"
      ],
      "metadata": {
        "id": "ARjn8je3cy4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "I am converting the target variable into binary (1 = good credit, 0 = bad credit).\n",
        "Then I will label encode all categorical columns and scale the features using StandardScaler"
      ],
      "metadata": {
        "id": "-QnklIiadA3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Converting the target to binary\n",
        "df['credit_risk'] = df['credit_risk'].map({1: 1, 2: 0})\n",
        "\n",
        "## Dropping rows with NaN values in the credit_risk column\n",
        "df.dropna(subset=['credit_risk'], inplace=True)\n",
        "\n",
        "## Label encoding all text columns\n",
        "label_encoder = LabelEncoder()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "## Separating features and target\n",
        "X = df.drop(\"credit_risk\", axis=1)\n",
        "y = df[\"credit_risk\"]\n",
        "\n",
        "## Scaling the features for better model performance and efficiency\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "54SBUq5bdHY1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting my Data\n",
        "\n",
        "I am using 80 percent for training and 20 percent for testing.\n",
        "Stratify is used to maintain class balance."
      ],
      "metadata": {
        "id": "syQtIeCjdP_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=1, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "-t5Kqkt9dT1r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and Training the Models Explanation\n",
        "\n",
        "The three classification models I will train are Logistic Regression, Decision Tree, Naive Bayes\n"
      ],
      "metadata": {
        "id": "0jV66xGEdtOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining and training my models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=3000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=1),\n",
        "    \"Naive Bayes\": GaussianNB()}"
      ],
      "metadata": {
        "id": "gDl_xqV1d3MV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluating the Models\n",
        "\n",
        "Now I will evaluate the models by using Accuracy, Recall, Confusion Matrix, 5-fold Cross-Validation Accuracy"
      ],
      "metadata": {
        "id": "_42LiPkxeRt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluating my models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, predictions)\n",
        "    rec = recall_score(y_test, predictions)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    report = classification_report(y_test, predictions, output_dict=True)\n",
        "    cv_score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
        "\n",
        "    results[name] = {\n",
        "        \"Accuracy\": round(acc, 3),\n",
        "        \"Recall\": round(rec, 3),\n",
        "        \"Cross-Validation Accuracy\": round(cv_score, 3),\n",
        "        \"Confusion Matrix\": cm\n",
        "    }"
      ],
      "metadata": {
        "id": "lL4AhG1reTQq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Final Results\n",
        "\n",
        "Below is a summary of the performance of each model.\n",
        "These results will be used in the final report to select the most appropriate model\n"
      ],
      "metadata": {
        "id": "mzbYEG2ye4gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Showing the results\n",
        "pprint.pprint(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RBGgl5lee-Te",
        "outputId": "18073fb8-3df0-4f95-84d5-261499a413ba"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Decision Tree': {'Accuracy': 0.72,\n",
            "                   'Confusion Matrix': array([[ 35,  25],\n",
            "       [ 31, 109]]),\n",
            "                   'Cross-Validation Accuracy': np.float64(0.684),\n",
            "                   'Recall': 0.779},\n",
            " 'Logistic Regression': {'Accuracy': 0.76,\n",
            "                         'Confusion Matrix': array([[ 31,  29],\n",
            "       [ 19, 121]]),\n",
            "                         'Cross-Validation Accuracy': np.float64(0.76),\n",
            "                         'Recall': 0.864},\n",
            " 'Naive Bayes': {'Accuracy': 0.76,\n",
            "                 'Confusion Matrix': array([[ 39,  21],\n",
            "       [ 27, 113]]),\n",
            "                 'Cross-Validation Accuracy': np.float64(0.721),\n",
            "                 'Recall': 0.807}}\n"
          ]
        }
      ]
    }
  ]
}